{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "🚀 Dataset Toolkit - Gelbooru Scraper & Data Prep in Colab🤖✨\n",
        "\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "*   **🌐 Gelbooru Scraper:**  Download images and associated tags directly from Gelbooru based on your search queries, with extensive filtering options.\n",
        "*   **🏷️ Tag Management & Cleaning:** Tools to clean and standardize your dataset tags, including:\n",
        "    *   Removing duplicate tags\n",
        "    *   Cleaning tag structure (spaces, parentheses)\n",
        "    *   Replacing specific tags\n",
        "    *   Removing unwanted tags\n",
        "    *   Auto-tagging images using WD Tagger v3\n",
        "*   **👯 Duplicate Image Finder:**  Identify and remove visually similar or semantically duplicate images to ensure dataset quality.\n",
        "*   **🔍 Tag Analyzer:**  Gain insights into your dataset's tags with frequency analysis, category breakdowns, and powerful search capabilities, including wildcard search.\n",
        "*   **🧹 Dataset Cleaner:**  Remove orphaned tag files, video files, non-image files, and images with extreme aspect ratios or resolutions.\n",
        "*   **🗂️ File Management:**  Easily move or copy your processed dataset within Google Drive and count dataset statistics.\n",
        "\n",
        "\n",
        "\n",
        "| |Github|\n",
        "|:--|:-:|\n",
        "| 🏠 **Profile** | [![GitHub](https://img.shields.io/badge/GitHub-%23121011.svg?logo=github&logoColor=white)](https://github.com/xLegende)|\n",
        "| 📘 **Repos** | [![GitHub](https://img.shields.io/badge/GitHub-%23121011.svg?logo=github&logoColor=white)](https://github.com/xLegende/dataset_colab)|"
      ],
      "metadata": {
        "id": "B4aDUXNNqwpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ⭕ Mount Drive and Define Directories\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# --- ANSI Color Codes ---\n",
        "COLOR_RESET = \"\\033[0m\"\n",
        "COLOR_SUCCESS = \"\\033[32m\"  # Green for success\n",
        "COLOR_INFO = \"\\033[36m\"     # Cyan for info messages\n",
        "\n",
        "# --- Emojis ---\n",
        "EMOJI_DRIVE = \" ☁️\" # Cloud for Drive\n",
        "EMOJI_FOLDER = \" 🗂️\" # Folder for Directories\n",
        "EMOJI_CHECKMARK = \" ✅\" # Checkmark for Success\n",
        "\n",
        "# Mount Google Drive\n",
        "print(f\"{EMOJI_DRIVE}{COLOR_INFO} Mounting Google Drive...{COLOR_RESET}\")\n",
        "drive.mount('/content/drive')\n",
        "print(f\"{EMOJI_CHECKMARK}{COLOR_SUCCESS} Google Drive mounted successfully!{COLOR_RESET}\")\n",
        "\n",
        "# Define base output directory in Google Drive - you can change this here or in the form below\n",
        "OUTPUT_BASE_DIR = \"/content/drive/MyDrive/\" #@param {type:\"string\"}\n",
        "if not OUTPUT_BASE_DIR:\n",
        "    OUTPUT_BASE_DIR = \"/content/drive/MyDrive/\"\n",
        "\n",
        "# Define subdirectory for images - you can change this here or in the form below\n",
        "subdirectory = \"images\" #@param {type:\"string\"}\n",
        "\n",
        "# Construct full directory path in Google Drive\n",
        "full_directory = os.path.join(OUTPUT_BASE_DIR, subdirectory)\n",
        "\n",
        "# Ensure base directory and subdirectory exist in Google Drive\n",
        "os.makedirs(OUTPUT_BASE_DIR, exist_ok=True)\n",
        "os.makedirs(full_directory, exist_ok=True)\n",
        "\n",
        "print(f\"\\n--- {EMOJI_FOLDER} Directories Initialized {EMOJI_FOLDER} ---\")\n",
        "print(f\"{EMOJI_FOLDER} Base output directory set to: {COLOR_INFO}{OUTPUT_BASE_DIR}{COLOR_RESET}\")\n",
        "print(f\"{EMOJI_FOLDER} Subdirectory for images: {COLOR_INFO}{subdirectory}{COLOR_RESET}\")\n",
        "print(f\"{EMOJI_FOLDER} Full output directory: {COLOR_INFO}{full_directory}{COLOR_RESET}\")\n",
        "print(f\"{EMOJI_CHECKMARK}{COLOR_SUCCESS} Directories initialized successfully!{COLOR_RESET}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "n9iVDSnEmux1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "P4h2hwL3iiTj"
      },
      "outputs": [],
      "source": [
        "#@title 🛃 Gelbooru Scraper\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "# No need to import drive or os here, already done in the init cell\n",
        "\n",
        "# Colab parameters with default values\n",
        "search_query = \"-lowres *res -video -mp4 -monochrome -greyscale score:>5 -*english* -speech_bubble -comic -*text* \" #@param {type:\"string\"}\n",
        "limit = 5 #@param {type:\"integer\"}\n",
        "# subdirectory = \"images\" # Removed, defined in init cell\n",
        "include_general_tags = False #@param {type:\"boolean\"}\n",
        "include_artist_tags = True #@param {type:\"boolean\"}\n",
        "include_metadata_tags = True #@param {type:\"boolean\"}\n",
        "include_character_tags = True #@param {type:\"boolean\"}\n",
        "include_copyright_tags = False #@param {type:\"boolean\"}\n",
        "filter_unknown_tags = True #@param {type:\"boolean\"}\n",
        "trigger_word = \"\" #@param {type:\"string\"}\n",
        "exclude_tags_str = \"tagme, commentary_request, commentary, translation_request, official_art, english_commentary, commission, artist_request, non-web_source, skeb_commission, paid_reward_available, second-party_source, hashtag-only_commentary, bad_id, bad_twitter_id, character_request, translated, partial_commentary, copyright_request, chinese_commentary, korean_commentary, mixed-language_commentary, third-party_edit, revision, photoshop_(medium), source_request, symbol-only_commentary, variant_set, textless_version, third-party_source, bad_pixiv_id, commissioner_upload, making-of_available, clothing_request, self-upload, slideshow_animation, novel_illustration,photo-referenced, duplicate, huge_filesize, corrupted_twitter_file,flower_request, pixiv_commission, check_copyright, bad_link, food_request, vgen_commission, check_character, large_variant_set, ai-assisted, bad_fanbox_id, clip_studio_paint_(medium), paid_reward, check_artist, bad_source, fantia_reward, nude_filter, protected_link, inactive_account, derivative_work, artist_collaboration, image_sample, webp-to-png_conversion, weapon_request, psd_available, check_commentary, md5_mismatch, spoilers, archived_source, alt_text, merchandise_available, annotated, pixiv_sample, hair_ornament_request, live2d,\" #@param {type:\"string\"}\n",
        "\n",
        "exclude_tags = [tag.strip() for tag in exclude_tags_str.split(',') if tag.strip()]\n",
        "\n",
        "def scrape_gelbooru(query, limit=100, directory=full_directory, include_general=True, include_artists=False, include_metadata=False, include_characters=False, include_copyright=False, filter_unknown=False, trigger_word=\"\", exclude_tags=[]): # Use full_directory directly\n",
        "    \"\"\"\n",
        "    Scrapes images from Gelbooru, providing options for a trigger word, tag exclusion,\n",
        "    artist tags, metadata tags, character tags, copyright tags, and filtering unknown tags.\n",
        "    Saves images and tags to Google Drive.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query.\n",
        "        limit (int): The maximum number of images to download.\n",
        "        directory (str): The directory to save images and tag files (relative to Google Drive base).\n",
        "        include_general (bool): Whether to include general tags.\n",
        "        include_artists (bool): Whether to include artist tags.\n",
        "        include_metadata (bool): Whether to include metadata tags.\n",
        "        include_characters (bool): Whether to include character tags.\n",
        "        include_copyright (bool): Whether to include copyright tags.\n",
        "        filter_unknown (bool): Whether to filter out tags of unknown type.\n",
        "        trigger_word (str): A trigger word to be added as the first tag.\n",
        "        exclude_tags (list): A list of tags to exclude from the tag file.\n",
        "    \"\"\"\n",
        "\n",
        "    api_url = \"https://gelbooru.com/index.php\"\n",
        "    page = 0\n",
        "    downloaded_count = 0\n",
        "\n",
        "    # Directory is already created in the init cell\n",
        "    print(f\"Saving images and tags to: {directory}\") # directory is now full_directory\n",
        "\n",
        "    while downloaded_count < limit:\n",
        "        params = {\n",
        "            \"page\": \"dapi\",\n",
        "            \"s\": \"post\",\n",
        "            \"q\": \"index\",\n",
        "            \"tags\": query,\n",
        "            \"json\": 1,\n",
        "            \"pid\": page,\n",
        "            \"limit\": 100,\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(api_url, params=params)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if not data or 'post' not in data or not data['post']:\n",
        "                print(\"No more images found.\")\n",
        "                break\n",
        "\n",
        "            for post in data['post']:\n",
        "                if downloaded_count >= limit:\n",
        "                    break\n",
        "\n",
        "                image_url = post['file_url']\n",
        "                image_id = post['id']\n",
        "                base_filename = f\"{image_id}\"\n",
        "                image_filename = f\"{base_filename}{os.path.splitext(image_url)[1]}\"\n",
        "                image_path = os.path.join(directory, image_filename) # Use directory (full_directory)\n",
        "                tag_filename = f\"{base_filename}.txt\"\n",
        "                tag_path = os.path.join(directory, tag_filename) # Use directory (full_directory)\n",
        "\n",
        "                # Get tag information with categories using the API\n",
        "                tag_data = get_tag_data(post['tags'])\n",
        "                tags_to_write = []\n",
        "\n",
        "                # Add trigger word if provided\n",
        "                if trigger_word:\n",
        "                    tags_to_write.append(trigger_word)\n",
        "\n",
        "                for tag_info in tag_data:\n",
        "                    tag_name = tag_info['name']\n",
        "                    tag_type = tag_info['category']\n",
        "\n",
        "                    # Exclude tags based on the exclude_tags list\n",
        "                    if tag_name in exclude_tags:\n",
        "                        continue\n",
        "\n",
        "                    # Filter tags based on type and boolean flags\n",
        "                    if (tag_type == \"general\" and not include_general) or \\\n",
        "                       (tag_type == \"artist\" and not include_artists) or \\\n",
        "                       (tag_type == \"meta\" and not include_metadata) or \\\n",
        "                       (tag_type == \"character\" and not include_characters) or \\\n",
        "                       (tag_type == \"copyright\" and not include_copyright) or \\\n",
        "                       (filter_unknown and tag_type == \"unknown\"):  # Filter unknown tags\n",
        "                        continue\n",
        "                    else:\n",
        "                        tags_to_write.append(tag_name)\n",
        "\n",
        "                tags_string = ', '.join(tags_to_write)\n",
        "\n",
        "                try:\n",
        "                    image_response = requests.get(image_url, stream=True)\n",
        "                    image_response.raise_for_status()\n",
        "\n",
        "                    with open(image_path, 'wb') as image_file:\n",
        "                        for chunk in image_response.iter_content(chunk_size=8192):\n",
        "                            image_file.write(chunk)\n",
        "\n",
        "                    with open(tag_path, 'w') as tag_file:\n",
        "                        tag_file.write(tags_string)\n",
        "\n",
        "                    print(f\"Downloaded: {image_filename} with tags in {tag_filename} to '{subdirectory}'\")\n",
        "                    downloaded_count += 1\n",
        "\n",
        "                except requests.exceptions.RequestException as e:\n",
        "                    print(f\"Error downloading {image_url}: {e}\")\n",
        "\n",
        "                time.sleep(0.1)\n",
        "\n",
        "            page += 1\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching data from API: {e}\")\n",
        "            break\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error decoding JSON: {e}\")\n",
        "            break\n",
        "\n",
        "def get_tag_data(tags_string):\n",
        "    \"\"\"\n",
        "    Retrieves tag data with categories using the Gelbooru API.\n",
        "\n",
        "    Args:\n",
        "        tags_string (str): A space-separated string of tags.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries, where each dictionary represents a tag\n",
        "              and contains 'name' and 'category' keys.\n",
        "    \"\"\"\n",
        "\n",
        "    api_url = \"https://gelbooru.com/index.php\"\n",
        "    params = {\n",
        "        \"page\": \"dapi\",\n",
        "        \"s\": \"tag\",\n",
        "        \"q\": \"index\",\n",
        "        \"json\": 1,\n",
        "        \"names\": tags_string\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(api_url, params=params)\n",
        "        response.raise_for_status()\n",
        "        tag_data = response.json()\n",
        "\n",
        "        # Check if the response is a dictionary and convert it to a list\n",
        "        if isinstance(tag_data, dict) and 'tag' in tag_data:\n",
        "            tag_data = tag_data['tag']\n",
        "        elif not isinstance(tag_data, list):\n",
        "            print(f\"Unexpected API response format: {tag_data}\")\n",
        "            return []\n",
        "\n",
        "        # Map Gelbooru's numerical tag types to category names\n",
        "        tag_type_mapping = {\n",
        "            0: \"general\",\n",
        "            1: \"artist\",\n",
        "            3: \"copyright\",\n",
        "            4: \"character\",\n",
        "            5: \"meta\",\n",
        "        }\n",
        "\n",
        "        # Extract tag names and categories\n",
        "        tag_info = []\n",
        "        for tag in tag_data:\n",
        "            tag_info.append({\n",
        "                \"name\": tag[\"name\"],\n",
        "                \"category\": tag_type_mapping.get(tag[\"type\"], \"unknown\")\n",
        "            })\n",
        "\n",
        "        return tag_info\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching tag data from API: {e}\")\n",
        "        return []\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON for tag data: {e}\")\n",
        "        return []\n",
        "\n",
        "# Main execution using Colab parameters\n",
        "if __name__ == \"__main__\": # This block will only run if you run as a script, not in colab directly, so removing it\n",
        "    scrape_gelbooru(\n",
        "        search_query,\n",
        "        limit=limit,\n",
        "        # directory=subdirectory, # Removed, using full_directory directly in function definition\n",
        "        include_general=include_general_tags,\n",
        "        include_artists=include_artist_tags,\n",
        "        include_metadata=include_metadata_tags,\n",
        "        include_characters=include_character_tags,\n",
        "        include_copyright=include_copyright_tags,\n",
        "        filter_unknown=filter_unknown_tags,\n",
        "        trigger_word=trigger_word,\n",
        "        exclude_tags=exclude_tags\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🤖 Auto Tagger (WD Tagger v3)\n",
        "\n",
        "import os\n",
        "import timm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd # Import pandas\n",
        "from huggingface_hub import hf_hub_download # Import hf_hub_download\n",
        "from timm.data import create_transform, resolve_data_config # Import timm transforms\n",
        "from torch.nn import functional as F # Import F for sigmoid\n",
        "\n",
        "# No need to import drive or define directories again\n",
        "# Define directory using the initialized full_directory\n",
        "# full_directory is assumed to be defined in the \"Initialization Cell\"\n",
        "\n",
        "# Colab Parameters\n",
        "enable_auto_tagging = True  #@param {type:\"boolean\"}\n",
        "general_tag_threshold = 0.4  #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "character_tag_threshold = 0.8  #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "tag_saving_option = \"Overwrite\"  #@param [\"Overwrite\", \"Append\"]\n",
        "\n",
        "MODEL_REPO_MAP = { # Model repo map from example script\n",
        "    \"vit\": \"SmilingWolf/wd-vit-tagger-v3\",\n",
        "    \"swinv2\": \"SmilingWolf/wd-swinv2-tagger-v3\",\n",
        "    \"convnext\": \"SmilingWolf/wd-convnext-tagger-v3\",\n",
        "}\n",
        "MODEL_NAME = \"swinv2\" #@param [\"vit\", \"swinv2\", \"convnext\"]\n",
        "\n",
        "def load_wd_tagger(model_name_key=MODEL_NAME): # Modified to take model_name_key\n",
        "    \"\"\"Loads WD Tagger model using timm library.\"\"\"\n",
        "    try:\n",
        "        repo_id = MODEL_REPO_MAP.get(model_name_key) # Get repo_id from MODEL_REPO_MAP\n",
        "        model = timm.create_model(\"hf-hub:\" + repo_id).eval() # Load model from timm with hf-hub prefix\n",
        "        state_dict = timm.models.load_state_dict_from_hf(repo_id) # Load state_dict\n",
        "        model.load_state_dict(state_dict) # Load state_dict into model\n",
        "\n",
        "        print(f\"✅ WD Tagger ({model_name_key}) model loaded successfully.\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading WD Tagger ({model_name_key}) model: {e}\")\n",
        "        return None\n",
        "\n",
        "def pil_ensure_rgb(image: Image.Image) -> Image.Image:\n",
        "    # convert to RGB/RGBA if not already (deals with palette images etc.)\n",
        "    if image.mode not in [\"RGB\", \"RGBA\"]:\n",
        "        image = image.convert(\"RGBA\") if \"transparency\" in image.info else image.convert(\"RGB\")\n",
        "    # convert RGBA to RGB with white background\n",
        "    if image.mode == \"RGBA\":\n",
        "        canvas = Image.new(\"RGBA\", image.size, (255, 255, 255))\n",
        "        canvas.alpha_composite(image)\n",
        "        image = canvas.convert(\"RGB\")\n",
        "    return image\n",
        "\n",
        "def pil_pad_square(image: Image.Image) -> Image.Image:\n",
        "    w, h = image.size\n",
        "    # get the largest dimension so we can pad to a square\n",
        "    px = max(image.size)\n",
        "    # pad to square with white background\n",
        "    canvas = Image.new(\"RGB\", (px, px), (255, 255, 255))\n",
        "    canvas.paste(image, ((px - w) // 2, (px - h) // 2))\n",
        "    return canvas\n",
        "\n",
        "\n",
        "def load_labels_hf(repo_id): # Modified to take repo_id\n",
        "    \"\"\"Loads labels from HuggingFace Hub.\"\"\"\n",
        "\n",
        "    labels_path_hf = hf_hub_download(repo_id=repo_id, filename=\"selected_tags.csv\",  local_dir=\".\") # Download labels CSV\n",
        "\n",
        "    labels_df = pd.read_csv(labels_path_hf, usecols=[\"name\", \"category\"]) # Load labels using pandas\n",
        "    label_names = labels_df[\"name\"].tolist()\n",
        "    categories = labels_df[\"category\"].tolist()\n",
        "\n",
        "    labels = {}\n",
        "    labels['names'] = label_names\n",
        "    labels['rating'] = list(np.where(np.array(categories) == 9)[0])\n",
        "    labels['general'] = list(np.where(np.array(categories) == 0)[0])\n",
        "    labels['character'] = list(np.where(np.array(categories) == 4)[0])\n",
        "\n",
        "    return labels\n",
        "\n",
        "\n",
        "\n",
        "def get_tags(\n",
        "    probs,\n",
        "    labels,\n",
        "    gen_threshold: float,\n",
        "    char_threshold: float,\n",
        "):\n",
        "    # Convert indices+probs to labels\n",
        "    probs_list = list(zip(labels['names'], probs)) # Use labels dict\n",
        "\n",
        "    # First 4 labels are actually ratings\n",
        "    rating_labels = dict([probs_list[i] for i in labels['rating']]) # Use labels dict\n",
        "\n",
        "    # General labels, pick any where prediction confidence > threshold\n",
        "    gen_labels = [probs_list[i] for i in labels['general']] # Use labels dict\n",
        "    gen_labels = dict([x for x in gen_labels if x[1] > gen_threshold])\n",
        "    gen_labels = dict(sorted(gen_labels.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "    # Character labels, pick any where prediction confidence > threshold\n",
        "    char_labels = [probs_list[i] for i in labels['character']] # Use labels dict\n",
        "    char_labels = dict([x for x in char_labels if x[1] > char_threshold])\n",
        "    char_labels = dict(sorted(char_labels.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "    # Combine general and character labels, sort by confidence\n",
        "    combined_names = [x for x in gen_labels]\n",
        "    combined_names.extend([x for x in char_labels])\n",
        "\n",
        "    # Convert to a string suitable for use as a training caption\n",
        "    caption = \", \".join(combined_names)\n",
        "    taglist = caption.replace(\"_\", \" \").replace(\"(\", \"\\(\").replace(\")\", \"\\)\")\n",
        "\n",
        "    return caption, taglist, rating_labels, char_labels, gen_labels\n",
        "\n",
        "\n",
        "def auto_tag_image(image_path, model, labels, general_threshold, character_threshold, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"): # Modified to accept labels\n",
        "    \"\"\"Generates tags for an image using WD Tagger (timm).\"\"\"\n",
        "    try:\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        img_tensor = timm.data.transforms_factory.transforms_imagenet_eval(448)(image).unsqueeze(0).to(device) # CHANGED: Use 448 instead of 256\n",
        "        img_tensor = img_tensor[:, [2, 1, 0]] # RGB to BGR\n",
        "        model.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          output = model(img_tensor)\n",
        "          probs = F.sigmoid(output.cpu()).detach().numpy()[0] # Apply sigmoid and move to CPU\n",
        "\n",
        "        character_tags, general_tags = get_tags(probs=probs, labels=labels, gen_threshold=general_threshold, char_threshold=character_threshold)[3:] # Use get_tags function\n",
        "\n",
        "        character_tags = list(character_tags.keys()) # Extract tag names from dict\n",
        "        general_tags = list(general_tags.keys()) # Extract tag names from dict\n",
        "\n",
        "\n",
        "        return character_tags, general_tags\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error tagging image {image_path}: {e}\")\n",
        "        return [], []\n",
        "\n",
        "def save_tags(tag_path, character_tags, general_tags, tag_saving_option=\"Overwrite\"):\n",
        "    \"\"\"Saves generated tags to a .txt file.\"\"\"\n",
        "    try:\n",
        "        tags_string = ', '.join(character_tags + general_tags)\n",
        "\n",
        "        if tag_saving_option == \"Overwrite\":\n",
        "          with open(tag_path, 'w') as f:\n",
        "                f.write(tags_string)\n",
        "        elif tag_saving_option == \"Append\":\n",
        "          with open(tag_path, 'a') as f:\n",
        "            if os.path.getsize(tag_path) > 0: # Check if tag file is empty\n",
        "                f.write(', ')\n",
        "            f.write(tags_string)\n",
        "        else:\n",
        "            print(f\"⚠️ Warning: Invalid tag saving option selected: '{tag_saving_option}'. Tags not saved for {tag_path}\")\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error saving tags to {tag_path}: {e}\")\n",
        "        return False\n",
        "\n",
        "if enable_auto_tagging:\n",
        "  print(\"🤖 Starting auto-tagging process...\")\n",
        "\n",
        "  # Install pandas if not already installed\n",
        "  try:\n",
        "      import pandas as pd\n",
        "  except ImportError:\n",
        "      print(\"Installing pandas...\")\n",
        "      !pip install pandas -q\n",
        "      import pandas as pd\n",
        "\n",
        "  # Install timm if not already installed\n",
        "  try:\n",
        "      import timm\n",
        "  except ImportError:\n",
        "      print(\"Installing timm...\")\n",
        "      !pip install timm -q\n",
        "      import timm\n",
        "\n",
        "\n",
        "  model = load_wd_tagger() # Load model using timm\n",
        "  labels = load_labels_hf(repo_id=MODEL_REPO_MAP.get(MODEL_NAME)) # Load labels\n",
        "\n",
        "\n",
        "  if model and labels: # Check if both model and labels loaded successfully\n",
        "    image_extensions = ['.jpg', '.jpeg', '.png', '.webp'] # Add GIF here if needed\n",
        "    tagged_count = 0\n",
        "    tag_errors = 0\n",
        "\n",
        "\n",
        "    print(f\"✅ Labels file 'selected_tags.csv' loaded.\")\n",
        "\n",
        "    for filename in os.listdir(full_directory):\n",
        "      is_image = False\n",
        "      for ext in image_extensions:\n",
        "          if filename.lower().endswith(ext):\n",
        "              is_image = True\n",
        "              break\n",
        "      if is_image:\n",
        "        image_path = os.path.join(full_directory, filename)\n",
        "        tag_filename = os.path.splitext(filename)[0] + \".txt\"\n",
        "        tag_path = os.path.join(full_directory, tag_filename)\n",
        "\n",
        "        character_tags, general_tags = auto_tag_image(image_path, model, labels, general_tag_threshold, character_tag_threshold) # Pass labels to auto_tag_image\n",
        "\n",
        "        if save_tags(tag_path, character_tags, general_tags, tag_saving_option):\n",
        "          tagged_count += 1\n",
        "          print(f\"✅ Tagged: {filename} -> tags saved to {tag_filename}\")\n",
        "        else:\n",
        "          tag_errors += 1\n",
        "          print(f\"❌ Error: Failed to save tags for {filename}\")\n",
        "\n",
        "\n",
        "        print(\"\\n--- Auto Tagging Summary ---\")\n",
        "        print(f\"Images tagged: {tagged_count}\")\n",
        "        print(f\"Tag errors: {tag_errors}\")\n",
        "        print(\"✅ Auto-tagging process completed .\")\n",
        "  else:\n",
        "      print(\"❌ Auto-tagging process cancelled due to model loading error (timm) or labels loading error.\") # Modified error message\n",
        "\n",
        "else:\n",
        "  print(\"Auto-tagging process is disabled. Please enable the checkbox to run this feature.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AUKKfPAw7tOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 👯 Find Semantic Duplicates (using FiftyOne & CLIP)\n",
        "\n",
        "import os\n",
        "from google.colab import output\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# No need to import drive or define directories again\n",
        "# Define directory using the initialized full_directory\n",
        "# full_directory is assumed to be defined in the \"Initialization Cell\"\n",
        "\n",
        "# Feature Checkboxes\n",
        "enable_semantic_duplicate_finding = True #@param {type:\"boolean\"}\n",
        "# Cosine similarity threshold\n",
        "similarity_threshold = 0.96 #@param {type:\"slider\", min:0.90, max:0.999, step:0.001}\n",
        "\n",
        "images_folder = full_directory # Use the full_directory defined in initialization\n",
        "root_dir = OUTPUT_BASE_DIR # Use the OUTPUT_BASE_DIR defined in initialization\n",
        "project_subfolder = \"cleaned_dataset\" # Subfolder for exported cleaned dataset\n",
        "\n",
        "step3_installed_flag = False # Flag to track installation\n",
        "\n",
        "if enable_semantic_duplicate_finding:\n",
        "\n",
        "    os.chdir(root_dir)\n",
        "    model_name = \"clip-vit-base32-torch\"\n",
        "    supported_types = (\".png\", \".jpg\", \".jpeg\")\n",
        "    img_count = len(os.listdir(images_folder))\n",
        "    batch_size = min(250, img_count)\n",
        "\n",
        "    if not step3_installed_flag:\n",
        "      print(\"🏭 Installing dependencies for semantic duplicate finding...\\n\")\n",
        "      !pip -q install fiftyone ftfy\n",
        "      !pip -q install fiftyone-db-ubuntu2204\n",
        "      if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "        clear_output()\n",
        "        step3_installed_flag = True\n",
        "        print(\"✅ Dependencies installed.\")\n",
        "      else:\n",
        "        print(\"❌ Error installing dependencies, attempting to continue anyway...\")\n",
        "\n",
        "    import numpy as np\n",
        "    import fiftyone as fo\n",
        "    import fiftyone.zoo as foz\n",
        "    from fiftyone import ViewField as F\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "    non_images = [f for f in os.listdir(images_folder) if not f.lower().endswith(supported_types) and not f.lower().endswith(\".txt\")] # IGNORE .txt files\n",
        "    if non_images:\n",
        "      print(f\"💥 Error: Found non-image file {non_images[0]} - This program doesn't allow it. Please use the '🚮 Clean folder' cell to remove non-image files.\")\n",
        "    elif img_count == 0:\n",
        "      print(f\"💥 Error: No images found in {images_folder}\")\n",
        "    else:\n",
        "      print(\"\\n💿 Analyzing dataset with FiftyOne...\\n\")\n",
        "      dataset = fo.Dataset.from_dir(images_folder, dataset_type=fo.types.ImageDirectory)\n",
        "\n",
        "      if len(dataset) == 0: # ADDED CHECK: Check if FiftyOne dataset is empty after loading\n",
        "          print(f\"💥 Error: FiftyOne could not load any valid images from '{images_folder}'.\")\n",
        "          print(\"Possible reasons: No supported image files, corrupted image files, or file access issues.\")\n",
        "          print(\"Please check your image directory and ensure it contains valid and supported image files (e.g., .png, .jpg, .jpeg, .gif).\")\n",
        "          enable_semantic_duplicate_finding = False # Stop further processing if no images loaded\n",
        "      else: # Proceed only if dataset is not empty\n",
        "          model = foz.load_zoo_model(model_name)\n",
        "          embeddings = dataset.compute_embeddings(model, batch_size=batch_size)\n",
        "\n",
        "          batch_embeddings = np.array_split(embeddings, batch_size)\n",
        "          similarity_matrices = []\n",
        "          max_size_x = max(array.shape[0] for array in batch_embeddings)\n",
        "          max_size_y = max(array.shape[1] for array in batch_embeddings)\n",
        "\n",
        "          for i, batch_embedding in enumerate(batch_embeddings):\n",
        "            if batch_embedding.shape[0] > 0: # ADDED CHECK: Skip empty batches\n",
        "              similarity = cosine_similarity(batch_embedding)\n",
        "              #Pad 0 for np.concatenate\n",
        "              padded_array = np.zeros((max_size_x, max_size_y))\n",
        "              padded_array[0:similarity.shape[0], 0:similarity.shape[1]] = similarity\n",
        "              similarity_matrices.append(padded_array)\n",
        "            else:\n",
        "              print(f\"⚠️ Warning: Skipping empty batch {i} during similarity calculation.\") # Optional warning\n",
        "\n",
        "          if similarity_matrices: # Proceed only if similarity_matrices is not empty\n",
        "            similarity_matrix = np.concatenate(similarity_matrices, axis=0)\n",
        "            similarity_matrix = similarity_matrix[0:embeddings.shape[0], 0:embeddings.shape[0]]\n",
        "\n",
        "            similarity_matrix = cosine_similarity(embeddings)\n",
        "            similarity_matrix -= np.identity(len(similarity_matrix))\n",
        "\n",
        "            dataset.match(F(\"max_similarity\") > similarity_threshold)\n",
        "            dataset.tags = [\"delete\", \"has_duplicates\"]\n",
        "\n",
        "            id_map = [s.id for s in dataset.select_fields([\"id\"])]\n",
        "            samples_to_remove = set()\n",
        "            samples_to_keep = set()\n",
        "\n",
        "            for idx, sample in enumerate(dataset):\n",
        "              if sample.id not in samples_to_remove:\n",
        "                # Keep the first instance of two duplicates\n",
        "                samples_to_keep.add(sample.id)\n",
        "\n",
        "                dup_idxs = np.where(similarity_matrix[idx] > similarity_threshold)[0]\n",
        "                for dup in dup_idxs:\n",
        "                    # We kept the first instance so remove all other duplicates\n",
        "                    samples_to_remove.add(id_map[dup])\n",
        "\n",
        "                if len(dup_idxs) > 0:\n",
        "                    sample.tags.append(\"has_duplicates\")\n",
        "                    sample.save()\n",
        "              else:\n",
        "                sample.tags.append(\"delete\")\n",
        "                sample.save()\n",
        "\n",
        "            clear_output()\n",
        "\n",
        "            # sidebar_groups = fo.DatasetAppConfig.default_groups(dataset) # Corrected: default_groups instead of default_sidebar_groups for FiftyOne > 0.17 # REMOVED due to AttributeError\n",
        "            # for group in sidebar_groups[1:]:\n",
        "            #   group.expanded = False\n",
        "            # dataset.app_config.sidebar_groups = sidebar_groups # REMOVED due to AttributeError\n",
        "            dataset.save()\n",
        "            session = fo.launch_app(dataset)\n",
        "\n",
        "            print(\"❗ Wait a minute for the FiftyOne session to load. If it doesn't load after a few minutes:\")\n",
        "            print(\"- Try enabling cookies and removing tracking protection for the Google Colab website in your browser settings, as they may interfere with FiftyOne.\")\n",
        "            print(\"- If issues persist, check the FiftyOne documentation for Colab troubleshooting.\")\n",
        "            print(\"❗ Once loaded, you'll see a grid of your images in the FiftyOne App.\")\n",
        "            print(\"❗ To visualize images marked for deletion, enable \\\"sample tags\\\" in the left sidebar and select 'delete'.\")\n",
        "            print(\"❗ You can manually mark more images for deletion by selecting them and pressing the tag icon at the top, then adding the 'delete' tag.\")\n",
        "            input(\"⭕ When you're done reviewing and tagging in the FiftyOne App, press Enter here to save changes and remove duplicates: \")\n",
        "\n",
        "            print(\"💾 Saving changes and removing marked duplicates...\")\n",
        "\n",
        "            marked = [s for s in dataset if \"delete\" in s.tags]\n",
        "            dataset.delete_samples(marked)\n",
        "            previous_folder = images_folder[:images_folder.rfind(\"/\")]\n",
        "            dataset.export(export_dir=os.path.join(images_folder, project_subfolder), dataset_type=fo.types.ImageDirectory)\n",
        "\n",
        "            temp_suffix = \"_temp\"\n",
        "            !mv {images_folder} {images_folder}{temp_suffix}\n",
        "            !mv {images_folder}{temp_suffix}/{project_subfolder} {images_folder}\n",
        "            !rm -r {images_folder}{temp_suffix}\n",
        "\n",
        "            session.refresh()\n",
        "            fo.close_app()\n",
        "            clear_output()\n",
        "\n",
        "            print(f\"\\n✅ Removed {len(marked)} images marked as duplicates from dataset. You now have {len(os.listdir(images_folder))} images.\")\n",
        "            print(\"✅ FiftyOne session closed.\")\n",
        "          else:\n",
        "            print(\"⚠️ Warning: No similarity matrices calculated. Possible issue with embeddings or batch processing.\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"Semantic duplicate image finding is disabled. Check the 'enable_semantic_duplicate_finding' checkbox to run.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jJW7YnFEo9lD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🔄 Clean Dataset (Run after scraping)\n",
        "\n",
        "import os\n",
        "from PIL import Image # Import Pillow for image operations\n",
        "# No need to import drive here, already done in the init cell\n",
        "\n",
        "# Define directory using the initialized full_directory\n",
        "# OUTPUT_BASE_DIR = \"/content/drive/MyDrive/\" # Removed, already defined and used in init cell\n",
        "# subdirectory = \"images\" # Removed, already defined and used in init cell\n",
        "# full_directory = os.path.join(OUTPUT_BASE_DIR, subdirectory) # Removed, already defined in init cell\n",
        "\n",
        "# Feature Checkboxes\n",
        "remove_duplicate_tags = True #@param {type:\"boolean\"}\n",
        "remove_video_files = True #@param {type:\"boolean\"}\n",
        "clean_tag_structure = True #@param {type:\"boolean\"}\n",
        "remove_specific_tags = False #@param {type:\"boolean\"}\n",
        "tags_to_remove_str = \"deleteme\" #@param {type:\"string\"}\n",
        "remove_aspect_ratio_outliers = True #@param {type:\"boolean\"}\n",
        "max_aspect_ratio_threshold = 0.25 #@param {type:\"slider\", min:0.01, max:0.5, step:0.01}\n",
        "downscale_images = True #@param {type:\"boolean\"}\n",
        "downscale_size = 1024 #@param {type:\"slider\", min:512, max:2048, step:64}\n",
        "\n",
        "tags_to_remove = [tag.strip() for tag in tags_to_remove_str.split(',') if tag.strip()]\n",
        "\n",
        "TEXT_EMOJI_PATTERNS = [\"@_@\", \"^_^\", \"o_o\", \"x_x\", \">_<\", \"u_u\", \"-_-\"] # Add more if needed\n",
        "\n",
        "# --- ANSI Color Codes ---\n",
        "COLOR_RESET = \"\\033[0m\"\n",
        "COLOR_INFO = \"\\033[36m\"     # Cyan for info messages\n",
        "COLOR_WARNING = \"\\033[33m\"  # Yellow for warnings\n",
        "COLOR_SUCCESS = \"\\033[32m\"  # Green for success\n",
        "\n",
        "# --- Emojis ---\n",
        "EMOJI_CLEAN = \"🔄\"\n",
        "EMOJI_VIDEO = \"🎬\"\n",
        "EMOJI_TAGS = \"🏷️\"\n",
        "EMOJI_DUPLICATE = \"👯\"\n",
        "EMOJI_ASPECT_RATIO = \"📏\"\n",
        "EMOJI_DOWNSCALE = \"⬇️\"\n",
        "EMOJI_ORPHANED_TAGS = \"💔\"\n",
        "EMOJI_SUMMARY = \"📊\"\n",
        "EMOJI_CHECKMARK = \"✅\"\n",
        "\n",
        "def clean_tag_text(tag_string):\n",
        "    \"\"\"Cleans up the tag string, but preserves underscores in text emojis.\"\"\"\n",
        "    tag_string = tag_string.replace('\\\\(', '(').replace('\\\\)', ')')\n",
        "\n",
        "    is_emoji = False\n",
        "    for emoji_pattern in TEXT_EMOJI_PATTERNS:\n",
        "        if tag_string == emoji_pattern:\n",
        "            is_emoji = True\n",
        "            break\n",
        "\n",
        "    if not is_emoji:\n",
        "        tag_string = tag_string.replace('_', ' ')\n",
        "\n",
        "    tag_string = tag_string.replace('(', '\\(').replace(')', '\\)')\n",
        "    return tag_string\n",
        "\n",
        "def remove_duplicate_tags_from_file(tag_path):\n",
        "    \"\"\"Removes duplicate tags from a tag file.\"\"\"\n",
        "    try:\n",
        "        with open(tag_path, 'r') as f:\n",
        "            tags = f.read().strip()\n",
        "        if not tags: # if file is empty\n",
        "            return False\n",
        "\n",
        "        tag_list = [tag.strip() for tag in tags.split(',')]\n",
        "        unique_tags = sorted(list(set(tag_list))) # Remove duplicates and sort\n",
        "        updated_tag_string = ', '.join(unique_tags)\n",
        "\n",
        "        with open(tag_path, 'w') as f:\n",
        "            f.write(updated_tag_string)\n",
        "        return True # Modification happened\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing tag file {tag_path} for duplicate removal: {e}\")\n",
        "        return False\n",
        "\n",
        "def clean_tag_structure_in_file(tag_path):\n",
        "    \"\"\"Cleans the tag structure within a tag file.\"\"\"\n",
        "    try:\n",
        "        with open(tag_path, 'r') as f:\n",
        "            tags = f.read().strip()\n",
        "        if not tags: # if file is empty\n",
        "            return False\n",
        "\n",
        "        cleaned_tags = [clean_tag_text(tag.strip()) for tag in tags.split(',')]\n",
        "        updated_tag_string = ', '.join(cleaned_tags)\n",
        "\n",
        "        with open(tag_path, 'w') as f:\n",
        "            f.write(updated_tag_string)\n",
        "        return True # Modification happened\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing tag file {tag_path} for tag structure cleaning: {e}\")\n",
        "        return False\n",
        "\n",
        "def remove_specific_tags_from_file(tag_path, tags_to_remove):\n",
        "    \"\"\"Removes specific tags from a tag file.\"\"\"\n",
        "    try:\n",
        "        with open(tag_path, 'r') as f:\n",
        "            tags_string = f.read().strip()\n",
        "        if not tags_string: # if file is empty\n",
        "            return False\n",
        "\n",
        "        tag_list = [tag.strip() for tag in tags_string.split(',')]\n",
        "        filtered_tags = [tag for tag in tag_list if tag not in tags_to_remove]\n",
        "        updated_tag_string = ', '.join(filtered_tags)\n",
        "\n",
        "        with open(tag_path, 'w') as f:\n",
        "            f.write(updated_tag_string)\n",
        "        return True # Modification happened\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing tag file {tag_path} for specific tag removal: {e}\")\n",
        "        return False\n",
        "\n",
        "def remove_aspect_ratio_outliers_from_file(image_path, threshold):\n",
        "    \"\"\"Removes images with extreme aspect ratios.\"\"\"\n",
        "    try:\n",
        "        img = Image.open(image_path)\n",
        "        width, height = img.size\n",
        "        aspect_ratio_wh = width / height\n",
        "        aspect_ratio_hw = height / width\n",
        "\n",
        "        if aspect_ratio_wh > (1 / threshold) or aspect_ratio_hw > (1 / threshold): # e.g., if threshold is 0.1, then > 10\n",
        "            return True # Image is an aspect ratio outlier\n",
        "        return False # Image is within aspect ratio threshold\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {image_path} for aspect ratio check: {e}\")\n",
        "        return False # Assume not an outlier in case of error\n",
        "\n",
        "def downscale_image(image_path, target_size):\n",
        "    \"\"\"Downscales an image so its smallest dimension is target_size, if necessary.\"\"\"\n",
        "    try:\n",
        "        img = Image.open(image_path)\n",
        "        width, height = img.size\n",
        "\n",
        "        if min(width, height) > target_size:\n",
        "            if width < height:\n",
        "                new_width = target_size\n",
        "                new_height = int(height * (new_width / width))\n",
        "            else:\n",
        "                new_height = target_size\n",
        "                new_width = int(width * (new_height / height))\n",
        "\n",
        "            resized_img = img.resize((new_width, new_height), Image.LANCZOS) # Use LANCZOS for high-quality downscaling\n",
        "            resized_img.save(image_path) # Overwrite the original image\n",
        "            return True # Image was downscaled\n",
        "        return False # Image was not downscaled\n",
        "    except Exception as e:\n",
        "        print(f\"Error downscaling image {image_path}: {e}\")\n",
        "        return False # Indicate downscaling failed\n",
        "\n",
        "\n",
        "def delete_orphan_tag_files(directory=full_directory, remove_video=False, clean_tags=False, remove_dupes=False, remove_spec_tags=False, tags_to_remove=[], remove_aspect_ratio=False, aspect_ratio_threshold=0.1, downscale_images=False, downscale_size=1024): # Use full_directory directly\n",
        "    \"\"\"\n",
        "    Deletes .txt files in the given directory that do not have a corresponding image file\n",
        "    and optionally removes video files, cleans tag structure and removes duplicate tags.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        print(f\"{COLOR_WARNING}Directory not found: {directory}{COLOR_RESET}\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n--- {EMOJI_CLEAN} Cleaning Dataset in Progress {EMOJI_CLEAN} ---\")\n",
        "    print(f\"Processing files in: {directory}\")\n",
        "    tag_files_deleted = 0\n",
        "    video_files_deleted = 0\n",
        "    tags_cleaned_count = 0\n",
        "    dupes_removed_count = 0\n",
        "    spec_tags_removed_count = 0\n",
        "    aspect_ratio_outliers_removed_count = 0\n",
        "    downscaled_images_count = 0\n",
        "\n",
        "    video_extensions = ['.mp4', '.gif', '.webm', '.mov', '.avi'] # Add more video extensions if needed\n",
        "    image_extensions = ['.jpg', '.jpeg', '.png', '.webp'] # Image extensions for aspect ratio check\n",
        "\n",
        "    if remove_video:\n",
        "        print(f\"{EMOJI_VIDEO} Removing video files...\")\n",
        "        for filename in os.listdir(directory):\n",
        "            for ext in video_extensions:\n",
        "                if filename.lower().endswith(ext):\n",
        "                    video_path = os.path.join(directory, filename)\n",
        "                    tag_filename = os.path.splitext(filename)[0] + \".txt\"\n",
        "                    tag_path = os.path.join(directory, tag_path) # Corrected typo tag_path instead of filename\n",
        "\n",
        "                    try:\n",
        "                        os.remove(video_path)\n",
        "                        video_files_deleted += 1\n",
        "                        if os.path.exists(tag_path):\n",
        "                            os.remove(tag_path) # Delete tag file too if video is removed\n",
        "                            tag_files_deleted += 1 # Count as orphaned tag deleted in video removal\n",
        "                        print(f\"Deleted video file: {filename} and associated tag file (if any).\")\n",
        "                    except OSError as e:\n",
        "                        print(f\"Error deleting {filename}: {e}\")\n",
        "                        continue # Continue to next file even if error\n",
        "\n",
        "    if clean_tags:\n",
        "        print(f\"{EMOJI_TAGS} Cleaning tag structure in tag files...\")\n",
        "        for filename in os.listdir(directory):\n",
        "            if filename.endswith(\".txt\"):\n",
        "                tag_path = os.path.join(directory, filename)\n",
        "                if clean_tag_structure_in_file(tag_path):\n",
        "                    tags_cleaned_count += 1\n",
        "\n",
        "    if remove_dupes:\n",
        "        print(f\"{EMOJI_DUPLICATE} Removing duplicate tags in tag files...\")\n",
        "        for filename in os.listdir(directory):\n",
        "            if filename.endswith(\".txt\"):\n",
        "                tag_path = os.path.join(directory, filename)\n",
        "                if remove_duplicate_tags_from_file(tag_path):\n",
        "                    dupes_removed_count += 1\n",
        "\n",
        "    if remove_spec_tags:\n",
        "        print(f\"{EMOJI_TAGS} Removing specific tags: {tags_to_remove} from tag files...\")\n",
        "        for filename in os.listdir(directory):\n",
        "            if filename.endswith(\".txt\"):\n",
        "                tag_path = os.path.join(directory, filename)\n",
        "                if remove_specific_tags_from_file(tag_path, tags_to_remove):\n",
        "                    spec_tags_removed_count += 1\n",
        "\n",
        "    if remove_aspect_ratio:\n",
        "        print(f\"{EMOJI_ASPECT_RATIO} Removing images with extreme aspect ratios (threshold: 1:{1/aspect_ratio_threshold:.0f})...\")\n",
        "        for filename in os.listdir(directory):\n",
        "            for ext in image_extensions:\n",
        "                if filename.lower().endswith(ext):\n",
        "                    image_path = os.path.join(directory, filename)\n",
        "                    tag_filename = os.path.splitext(filename)[0] + \".txt\"\n",
        "                    tag_path = os.path.join(directory, tag_filename)\n",
        "\n",
        "                    if remove_aspect_ratio_outliers_from_file(image_path, aspect_ratio_threshold):\n",
        "                        try:\n",
        "                            os.remove(image_path)\n",
        "                            aspect_ratio_outliers_removed_count += 1\n",
        "                            if os.path.exists(tag_path):\n",
        "                                os.remove(tag_path) # Delete tag file too if image is removed\n",
        "                                tag_files_deleted += 1 # Count as orphaned tag deleted in image removal\n",
        "                            print(f\"Deleted image with extreme aspect ratio: {filename} and associated tag file (if any).\")\n",
        "                        except OSError as e:\n",
        "                            print(f\"Error deleting {filename}: {e}\")\n",
        "                        break # Only process image files once\n",
        "\n",
        "    if downscale_images:\n",
        "        print(f\"{EMOJI_DOWNSCALE} Downscaling images to max dimension: {downscale_size}...\")\n",
        "        for filename in os.listdir(directory):\n",
        "            for ext in image_extensions:\n",
        "                if filename.lower().endswith(ext):\n",
        "                    image_path = os.path.join(directory, filename)\n",
        "                    if downscale_image(image_path, downscale_size):\n",
        "                        downscaled_images_count += 1\n",
        "                        print(f\"Downscaled image: {filename}\")\n",
        "\n",
        "\n",
        "    print(\"Checking for orphaned text files...\")\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            base_filename = os.path.splitext(filename)[0]\n",
        "            tag_path = os.path.join(directory, filename)\n",
        "            image_exists = False\n",
        "\n",
        "            #print(f\"\\nChecking tag file: {filename}\") # DEBUG PRINT\n",
        "            #print(f\"  Base filename: {base_filename}\") # DEBUG PRINT\n",
        "            #print(f\"  Tag path: {tag_path}\") # DEBUG PRINT\n",
        "\n",
        "            # Check for common image file extensions (excluding GIFs for orphan check as GIFs can be videos)\n",
        "            for ext in ['.jpg', '.jpeg', '.png', '.webp']: # Exclude .gif here\n",
        "                image_filename = base_filename + ext\n",
        "                image_path = os.path.join(directory, image_filename)\n",
        "                #print(f\"  Checking for image: {image_filename}\") # DEBUG PRINT\n",
        "                #print(f\"  Image path: {image_path}\") # DEBUG PRINT\n",
        "                if os.path.exists(image_path):\n",
        "                    image_exists = True\n",
        "                    #print(f\"  Image EXISTS: {image_path}\") # DEBUG PRINT\n",
        "                    break # No need to check other extensions if image is found\n",
        "                #else:\n",
        "                    #print(f\"  Image DOES NOT EXIST: {image_path}\") # DEBUG PRINT\n",
        "\n",
        "            if not image_exists:\n",
        "                #print(f\"  Orphaned tag file FOUND: {filename}\") # DEBUG PRINT\n",
        "                try:\n",
        "                    os.remove(tag_path)\n",
        "                    print(f\"  Deleted orphaned tag file: {filename}\")\n",
        "                    tag_files_deleted += 1\n",
        "                except OSError as e:\n",
        "                    print(f\"Error deleting {filename}: {e}\")\n",
        "\n",
        "    print(\"\\n--- Cleanup Summary ---\")\n",
        "    if video_files_deleted > 0:\n",
        "        print(f\"{EMOJI_VIDEO} Deleted video files: {video_files_deleted}\")\n",
        "    if tags_cleaned_count > 0:\n",
        "        print(f\"{EMOJI_TAGS} Cleaned tag structure in tag files: {tags_cleaned_count}\")\n",
        "    if dupes_removed_count > 0:\n",
        "        print(f\"{EMOJI_DUPLICATE} Removed duplicate tags from tag files: {dupes_removed_count}\")\n",
        "    if spec_tags_removed_count > 0:\n",
        "        print(f\"{EMOJI_TAGS} Removed specific tags from tag files: {spec_tags_removed_count}\")\n",
        "    if aspect_ratio_outliers_removed_count > 0:\n",
        "        print(f\"{EMOJI_ASPECT_RATIO} Removed images with extreme aspect ratios: {aspect_ratio_outliers_removed_count}\")\n",
        "    if downscaled_images_count > 0:\n",
        "        print(f\"{EMOJI_DOWNSCALE} Downscaled images to max dimension {downscale_size}px: {downscaled_images_count}\")\n",
        "    if tag_files_deleted > 0 and not remove_video and not remove_aspect_ratio: # Don't double count if tags were deleted during video or aspect ratio removal\n",
        "        print(f\"{EMOJI_ORPHANED_TAGS} Deleted orphaned tag files: {tag_files_deleted}\")\n",
        "    elif tag_files_deleted == 0 and not video_files_deleted and tags_cleaned_count == 0 and dupes_removed_count == 0 and spec_tags_removed_count == 0 and aspect_ratio_outliers_removed_count == 0 and downscaled_images_count == 0:\n",
        "        print(f\"{EMOJI_CHECKMARK} No orphaned tag files, video files, tag structure issues, duplicate tags, specific tags to remove, extreme aspect ratio images, or images to downscale found.\")\n",
        "    else:\n",
        "        print(f\"{EMOJI_ORPHANED_TAGS} No additional orphaned tag files found (after video/aspect ratio/downscaling removal).\")\n",
        "\n",
        "    print(f\"{EMOJI_CHECKMARK} {COLOR_SUCCESS}Dataset cleaning process completed.{COLOR_RESET}\")\n",
        "\n",
        "\n",
        "# Run the function with checkbox parameters\n",
        "delete_orphan_tag_files(directory=full_directory, remove_video=remove_video_files, clean_tags=clean_tag_structure, remove_dupes=remove_duplicate_tags, remove_spec_tags=remove_specific_tags, tags_to_remove=tags_to_remove, remove_aspect_ratio=remove_aspect_ratio_outliers, aspect_ratio_threshold=max_aspect_ratio_threshold, downscale_images=downscale_images, downscale_size=downscale_size) # Use full_directory"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WKL2Ecfeker4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🔁 Tag Replacer\n",
        "\n",
        "import os\n",
        "\n",
        "# No need to import drive or define directories again\n",
        "# Define directory using the initialized full_directory\n",
        "# full_directory is assumed to be defined in the \"Initialization Cell\"\n",
        "\n",
        "# Colab Parameters\n",
        "tags_to_replace_str = \"1girl\" #@param {type:\"string\"}\n",
        "replacement_tag = \"1woman\" #@param {type:\"string\"}\n",
        "case_insensitive = True #@param {type:\"boolean\"}\n",
        "\n",
        "tags_to_replace = [tag.strip() for tag in tags_to_replace_str.split(',') if tag.strip()]\n",
        "\n",
        "# --- ANSI Color Codes ---\n",
        "COLOR_RESET = \"\\033[0m\"\n",
        "COLOR_WARNING = \"\\033[33m\"  # Yellow for warnings\n",
        "COLOR_SUCCESS = \"\\033[32m\"  # Green for success\n",
        "\n",
        "# --- Emojis ---\n",
        "EMOJI_REPLACE = \"🔁\"\n",
        "EMOJI_WARNING = \"⚠️\"\n",
        "EMOJI_CHECKMARK = \"✅\"\n",
        "EMOJI_FILES = \"🗂️\"\n",
        "EMOJI_TAGS = \"🏷️\"\n",
        "\n",
        "def replace_tags_in_directory(directory=full_directory, tags_to_replace=[], replacement_tag=\"\", case_insensitive=True):\n",
        "    \"\"\"\n",
        "    Replaces specified tags with a new tag in all .txt files in the given directory, with enhanced output.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        print(f\"{COLOR_WARNING}Directory not found: {directory}{COLOR_RESET}\") # Color added\n",
        "        return\n",
        "\n",
        "    if not tags_to_replace or not replacement_tag:\n",
        "        print(f\"{EMOJI_WARNING} {COLOR_WARNING}Warning: No tags to replace or replacement tag specified. No changes will be made.{COLOR_RESET}\") # Emoji and color added\n",
        "        return\n",
        "\n",
        "    print(f\"\\n--- {EMOJI_REPLACE} Tag Replacement in Progress {EMOJI_REPLACE} ---\")\n",
        "    print(f\"{EMOJI_FOLDER} Processing directory: {directory}\")\n",
        "    print(f\"{EMOJI_TAGS} Tags to replace: {tags_to_replace}\")\n",
        "    print(f\"{EMOJI_TAGS} Replacement tag: '{replacement_tag}'\")\n",
        "    if case_insensitive:\n",
        "        print(\"Performing case-insensitive replacement.\")\n",
        "\n",
        "    files_modified_count = 0\n",
        "    tags_replaced_count = 0\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            tag_path = os.path.join(directory, filename)\n",
        "            try:\n",
        "                with open(tag_path, 'r') as f:\n",
        "                    tags_string = f.read().strip()\n",
        "                if not tags_string:\n",
        "                    continue # Skip empty tag files\n",
        "\n",
        "                tag_list = [tag.strip() for tag in tags_string.split(',')]\n",
        "                updated_tags = []\n",
        "                file_modified = False\n",
        "                for tag in tag_list:\n",
        "                    found_match = False\n",
        "                    for tag_to_replace in tags_to_replace:\n",
        "                        if case_insensitive:\n",
        "                            if tag.lower() == tag_to_replace.lower():\n",
        "                                updated_tags.append(replacement_tag)\n",
        "                                tags_replaced_count += 1\n",
        "                                file_modified = True\n",
        "                                found_match = True\n",
        "                                break # Break inner loop once a match is found for case-insensitive\n",
        "                        else:\n",
        "                            if tag == tag_to_replace:\n",
        "                                updated_tags.append(replacement_tag)\n",
        "                                tags_replaced_count += 1\n",
        "                                file_modified = True\n",
        "                                found_match = True\n",
        "                                break # Break inner loop once a match is found for case-sensitive\n",
        "                    if not found_match:\n",
        "                        updated_tags.append(tag) # Keep original tag if no replacement\n",
        "\n",
        "                if file_modified:\n",
        "                    updated_tag_string = ', '.join(updated_tags)\n",
        "                    with open(tag_path, 'w') as f:\n",
        "                        f.write(updated_tag_string)\n",
        "                    files_modified_count += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing tag file {filename}: {e}\")\n",
        "\n",
        "    print(f\"\\n--- {EMOJI_REPLACE} Tag Replacement Summary {EMOJI_REPLACE} ---\")\n",
        "    print(f\"{EMOJI_FILES} Files modified: {files_modified_count}\")\n",
        "    print(f\"{EMOJI_TAGS} Tags replaced: {tags_replaced_count}\")\n",
        "    print(f\"{EMOJI_CHECKMARK} {COLOR_SUCCESS}Tag replacement process completed.{COLOR_RESET}\")\n",
        "\n",
        "\n",
        "# Run tag replacement with Colab parameters\n",
        "replace_tags_in_directory(directory=full_directory, tags_to_replace=tags_to_replace, replacement_tag=replacement_tag, case_insensitive=case_insensitive)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ri7x8i-lnvfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 📈 Analyze Tags\n",
        "\n",
        "import os\n",
        "from collections import Counter\n",
        "# No need to import drive or define directories again\n",
        "import requests # Import requests for get_tag_data\n",
        "import json # Import json for get_tag_data\n",
        "\n",
        "# Define directory using the initialized full_directory\n",
        "# full_directory is assumed to be defined in the \"Initialization Cell\"\n",
        "\n",
        "# Analysis Parameters\n",
        "display_type = \"Top\" #@param [\"Top\", \"Bottom\", \"Alphabetical\"]\n",
        "num_tags_to_display = 100 #@param {type:\"integer\"}\n",
        "threshold_type = \"Minimum Frequency\" #@param [\"None\", \"Minimum Frequency\", \"Maximum Frequency\", \"Longer Than\", \"Shorter Than\"]\n",
        "threshold_value = 5 #@param {type:\"integer\"}\n",
        "analyze_category = \"All\" #@param [\"All\", \"General\", \"Artist\", \"Meta\", \"Character\", \"Copyright\", \"Unknown\"]\n",
        "\n",
        "# --- ANSI Color Codes ---\n",
        "COLOR_RESET = \"\\033[0m\"\n",
        "COLOR_UNKNOWN = \"\\033[31m\"    # Red - ADDED COLOR_UNKNOWN definition here\n",
        "COLOR_CATEGORY = {\n",
        "    \"general\": \"\\033[34m\",    # Blue\n",
        "    \"artist\": \"\\033[32m\",     # Green\n",
        "    \"meta\": \"\\033[33m\",       # Yellow\n",
        "    \"character\": \"\\033[35m\",  # Magenta\n",
        "    \"copyright\": \"\\033[36m\",  # Cyan\n",
        "    \"unknown\": COLOR_UNKNOWN,    # Red - Use COLOR_UNKNOWN here for \"unknown\" category too, for consistency\n",
        "}\n",
        "\n",
        "# --- Emojis ---\n",
        "EMOJI_SUMMARY = \"📊\"\n",
        "EMOJI_TOP_TAGS = \"🏆\"\n",
        "EMOJI_BOTTOM_TAGS = \"📉\"\n",
        "EMOJI_ALPHABETICAL_TAGS = \"🔤\" # Added emoji for alphabetical sort\n",
        "EMOJI_CATEGORY_BREAKDOWN = \"🗂️\"\n",
        "\n",
        "def unclean_tag_text(tag_string):\n",
        "    \"\"\"Reverts cleaned tag string back to original format for API lookup.\"\"\"\n",
        "    tag_string = tag_string.replace(' ', '_') # Replace spaces with underscores\n",
        "    tag_string = tag_string.replace('\\\\(', '(').replace('\\\\)', ')') # Remove backslashes from parentheses\n",
        "    return tag_string\n",
        "\n",
        "\n",
        "def analyze_tags(directory=full_directory, display_type=\"Top\", num_tags_to_display=10, threshold_type=\"None\", threshold_value=0, analyze_category=\"All\"): # Renamed params\n",
        "    \"\"\"\n",
        "    Analyzes tag frequencies in .txt files, with frequency and length thresholds, and category filtering.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        print(f\"Directory not found: {directory}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Analyzing tags in: {directory}\")\n",
        "    if analyze_category != \"All\":\n",
        "        print(f\"Filtering by category: {analyze_category}\")\n",
        "\n",
        "    tag_counts = Counter()\n",
        "    tag_files_analyzed = 0\n",
        "\n",
        "    all_tags_with_categories = {} # Store tags with their categories\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            tag_files_analyzed += 1\n",
        "            tag_path = os.path.join(directory, filename)\n",
        "            try:\n",
        "                with open(tag_path, 'r') as f:\n",
        "                    tags_string = f.read().strip()\n",
        "                    if tags_string:\n",
        "                        tags = [tag.strip() for tag in tags_string.split(',')]\n",
        "                        tag_counts.update(tags)\n",
        "                        # Unclean tags for API lookup to get categories\n",
        "                        uncleaned_tags_for_api = [unclean_tag_text(tag) for tag in tags]\n",
        "                        tag_categories = get_tag_categories(uncleaned_tags_for_api) # Get categories for UNCLEANED tags\n",
        "                        # Store category for each *cleaned* tag name\n",
        "                        for i, tag_name in enumerate(tags):\n",
        "                            original_tag_name_api = unclean_tag_text(tag_name) # Unclean tag to match API response\n",
        "                            all_tags_with_categories[tag_name] = tag_categories.get(original_tag_name_api, 'unknown') # Get category using unclean name\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading tag file {filename}: {e}\")\n",
        "\n",
        "    total_unique_tags = len(tag_counts)\n",
        "    print(f\"\\n--- {EMOJI_SUMMARY} Tag Analysis Summary {EMOJI_SUMMARY} ---\")\n",
        "    print(f\"Tag files analyzed: {tag_files_analyzed}\")\n",
        "    print(f\"Total unique tags found: {total_unique_tags}\")\n",
        "\n",
        "    filtered_tags = tag_counts.copy()\n",
        "\n",
        "    if threshold_type == \"Minimum Frequency\": # Renamed option\n",
        "        filtered_tags = Counter({tag: count for tag, count in tag_counts.items() if count >= threshold_value})\n",
        "        print(f\"Applying Minimum frequency threshold: {threshold_value}\")\n",
        "    elif threshold_type == \"Maximum Frequency\": # Renamed option\n",
        "        filtered_tags = Counter({tag: count for tag, count in tag_counts.items() if count <= threshold_value})\n",
        "        print(f\"Applying Maximum frequency threshold: {threshold_value}\")\n",
        "    elif threshold_type == \"Longer Than\": # New length threshold\n",
        "        filtered_tags = Counter({tag: count for tag, count in tag_counts.items() if len(tag) > threshold_value})\n",
        "        print(f\"Filtering tags longer than {threshold_value} characters.\")\n",
        "    elif threshold_type == \"Shorter Than\": # New length threshold\n",
        "        filtered_tags = Counter({tag: count for tag, count in tag_counts.items() if len(tag) < threshold_value})\n",
        "        print(f\"Filtering tags shorter than {threshold_value} characters.\")\n",
        "\n",
        "\n",
        "    if analyze_category != \"All\":\n",
        "        category_filtered_tags = Counter()\n",
        "        for tag, count in filtered_tags.items():\n",
        "            if all_tags_with_categories.get(tag) == analyze_category.lower():\n",
        "                category_filtered_tags[tag] = count\n",
        "        filtered_tags = category_filtered_tags\n",
        "\n",
        "\n",
        "    num_filtered_unique_tags = len(filtered_tags)\n",
        "    print(f\"Unique tags after filtering: {num_filtered_unique_tags}\")\n",
        "\n",
        "    if not filtered_tags:\n",
        "        print(\"\\nNo tags to display after applying filters.\")\n",
        "        return\n",
        "\n",
        "    if display_type == \"Top\":\n",
        "        most_common_tags = filtered_tags.most_common(num_tags_to_display)\n",
        "        print(f\"\\n--- {EMOJI_TOP_TAGS} Top {num_tags_to_display} Most Frequent Tags ({analyze_category} Category) {EMOJI_TOP_TAGS} ---\") # Emoji added\n",
        "    elif display_type == \"Bottom\":\n",
        "        least_common_tags_list = filtered_tags.most_common()[:-num_tags_to_display-1:-1] # Get least common, reverse order\n",
        "        least_common_tags = reversed(least_common_tags_list)\n",
        "        print(f\"\\n--- {EMOJI_BOTTOM_TAGS} Bottom {num_tags_to_display} Least Frequent Tags ({analyze_category} Category) {EMOJI_BOTTOM_TAGS} ---\") # Emoji added\n",
        "    elif display_type == \"Alphabetical\":\n",
        "        alphabetical_tags_list = sorted(filtered_tags.keys())\n",
        "        display_tag_list = [(tag, filtered_tags[tag]) for tag in alphabetical_tags_list[:num_tags_to_display]] # Get counts for sorted tags, limit to num_tags_to_display\n",
        "        print(f\"\\n--- {EMOJI_ALPHABETICAL_TAGS} Top {num_tags_to_display} Tags (Alphabetical Order, {analyze_category} Category) {EMOJI_ALPHABETICAL_TAGS} ---\") # Emoji added\n",
        "    else:\n",
        "        print(\"Invalid display type selected.\")\n",
        "        return\n",
        "\n",
        "    if display_type == \"Top\":\n",
        "      display_tag_list = most_common_tags\n",
        "    elif display_type == \"Bottom\":\n",
        "      display_tag_list = least_common_tags\n",
        "    elif display_type == \"Alphabetical\":\n",
        "      pass\n",
        "\n",
        "    for tag, count in display_tag_list:\n",
        "        category = all_tags_with_categories.get(tag, 'unknown')\n",
        "        color_code = COLOR_CATEGORY.get(category, COLOR_UNKNOWN)\n",
        "        print(f\"- {tag}: {count} ({color_code}{category.capitalize()}{COLOR_RESET})\")\n",
        "\n",
        "    print(f\"\\n--- {EMOJI_CATEGORY_BREAKDOWN} Tag Category Breakdown {EMOJI_CATEGORY_BREAKDOWN} ---\")\n",
        "    category_counts = Counter()\n",
        "    for tag in filtered_tags.keys():\n",
        "        category_counts[all_tags_with_categories.get(tag, 'unknown')] += filtered_tags[tag]\n",
        "\n",
        "    for category, count in category_counts.most_common():\n",
        "        color_code = COLOR_CATEGORY.get(category, COLOR_UNKNOWN)\n",
        "        print(f\"- {color_code}{category.capitalize()}{COLOR_RESET}: {count} tags\")\n",
        "\n",
        "\n",
        "def get_tag_categories(tags_list):\n",
        "    \"\"\"\n",
        "    Retrieves tag categories for a list of tags using the Gelbooru API.\n",
        "    Returns a dictionary mapping tag names to categories.\n",
        "    \"\"\"\n",
        "    tags_string = ' '.join(tags_list)\n",
        "    tag_data_list = get_tag_data(tags_string)\n",
        "\n",
        "    tag_categories = {}\n",
        "    for tag_info in tag_data_list:\n",
        "        tag_categories[tag_info['name']] = tag_info['category']\n",
        "    return tag_categories\n",
        "\n",
        "\n",
        "def get_tag_data(tags_string):\n",
        "    \"\"\"\n",
        "    Retrieves tag data with categories using the Gelbooru API.\n",
        "    (Copied from scraper cell for tag category analysis)\n",
        "    \"\"\"\n",
        "\n",
        "    api_url = \"https://gelbooru.com/index.php\"\n",
        "    params = {\n",
        "        \"page\": \"dapi\",\n",
        "        \"s\": \"tag\",\n",
        "        \"q\": \"index\",\n",
        "        \"json\": 1,\n",
        "        \"names\": tags_string\n",
        "    }\n",
        "\n",
        "    try:\n",
        "      response = requests.get(api_url, params=params)\n",
        "      response.raise_for_status()\n",
        "      tag_data = response.json()\n",
        "\n",
        "      # Check if the response is a dictionary and convert it to a list\n",
        "      if isinstance(tag_data, dict) and 'tag' in tag_data:\n",
        "        tag_data = tag_data['tag']\n",
        "      elif not isinstance(tag_data, list):\n",
        "        print(f\"Unexpected API response format: {tag_data}\")\n",
        "        return []\n",
        "\n",
        "      # Map Gelbooru's numerical tag types to category names\n",
        "      tag_type_mapping = {\n",
        "          0: \"general\",\n",
        "          1: \"artist\",\n",
        "          3: \"copyright\",\n",
        "          4: \"character\",\n",
        "          5: \"meta\",\n",
        "        }\n",
        "\n",
        "      # Extract tag names and categories\n",
        "      tag_info = []\n",
        "      for tag in tag_data:\n",
        "          tag_info.append({\n",
        "            \"name\": tag[\"name\"],\n",
        "            \"category\": tag_type_mapping.get(tag[\"type\"], \"unknown\")\n",
        "          })\n",
        "\n",
        "      return tag_info\n",
        "    except requests.exceptions.RequestException as e:\n",
        "      print(f\"Error fetching tag data: {e}\")\n",
        "      return []\n",
        "\n",
        "# Run tag analysis with Colab parameters\n",
        "analyze_tags(directory=full_directory, display_type=display_type, num_tags_to_display=num_tags_to_display, threshold_type=threshold_type, threshold_value=threshold_value, analyze_category=analyze_category)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JUmf87Wi1bgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🔍 Search for Tags\n",
        "\n",
        "import os\n",
        "\n",
        "# No need to import drive or define directories again\n",
        "# Define directory using the initialized full_directory\n",
        "# full_directory is assumed to be defined in the \"Initialization Cell\"\n",
        "\n",
        "# Colab Parameters\n",
        "search_tags_str = \"*girl\" #@param {type:\"string\"}\n",
        "case_insensitive_search = True #@param {type:\"boolean\"}\n",
        "list_files_with_tags = True #@param {type:\"boolean\"}\n",
        "\n",
        "search_tags = [tag.strip() for tag in search_tags_str.split(',') if tag.strip()]\n",
        "\n",
        "# --- ANSI Color Codes ---\n",
        "COLOR_RESET = \"\\033[0m\"\n",
        "COLOR_HIGHLIGHT = \"\\033[33m\" # Yellow highlight for search terms\n",
        "\n",
        "# --- Emojis ---\n",
        "EMOJI_SEARCH = \"🔍\"\n",
        "EMOJI_TAG = \"🏷️\"\n",
        "EMOJI_FILE = \"📄\"\n",
        "\n",
        "def search_tags_in_directory(directory=full_directory, search_tags=[], case_insensitive_search=True, list_files_with_tags=True): # list_files_with_tags defaults to True\n",
        "    \"\"\"\n",
        "    Searches for specified tags (with wildcard support) in .txt files, counts occurrences,\n",
        "    displays matched tags, and lists files with the tags and their contained tags.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        print(f\"Directory not found: {directory}\")\n",
        "        return\n",
        "\n",
        "    if not search_tags:\n",
        "        print(\"⚠️ Warning: No tags specified to search for.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n--- {EMOJI_SEARCH} Tag Search in Progress (Wildcard Enabled) {EMOJI_SEARCH} ---\")\n",
        "    print(f\"Searching for tags: {search_tags}\")\n",
        "    print(f\"Directory: {directory}\")\n",
        "    if case_insensitive_search:\n",
        "        print(\"Performing case-insensitive search.\")\n",
        "\n",
        "    tag_occurrences = {tag: 0 for tag in search_tags} # Initialize counts for each search tag\n",
        "    files_with_tags = {tag: {} for tag in search_tags} # Modified: Store files as dicts, tag: {filename: [matched_tags]}\n",
        "    matched_tags_list = {tag: [] for tag in search_tags} # Store matched tags for wildcard searches\n",
        "\n",
        "    files_analyzed_count = 0\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            files_analyzed_count += 1\n",
        "            tag_path = os.path.join(directory, filename)\n",
        "            try:\n",
        "                with open(tag_path, 'r') as f:\n",
        "                    tags_string = f.read().strip()\n",
        "                if not tags_string:\n",
        "                    continue # Skip empty tag files\n",
        "\n",
        "                tag_list = [tag.strip() for tag in tags_string.split(',')]\n",
        "\n",
        "                for search_tag in search_tags:\n",
        "                    file_matched_tags = [] # NEW: List to store tags matched in current file\n",
        "                    found_in_file = False\n",
        "                    for tag in tag_list:\n",
        "                        if \"*\" in search_tag: # Wildcard search logic\n",
        "                            prefix = search_tag.split(\"*\")[0]\n",
        "                            if case_insensitive_search:\n",
        "                                if tag.lower().startswith(prefix.lower()):\n",
        "                                    tag_occurrences[search_tag] += 1\n",
        "                                    matched_tags_list[search_tag].append(tag)\n",
        "                                    file_matched_tags.append(tag) # NEW: Add matched tag to file_matched_tags\n",
        "                                    found_in_file = True\n",
        "                            else:\n",
        "                                if tag.startswith(prefix):\n",
        "                                    tag_occurrences[search_tag] += 1\n",
        "                                    matched_tags_list[search_tag].append(tag)\n",
        "                                    file_matched_tags.append(tag) # NEW: Add matched tag to file_matched_tags\n",
        "                                    found_in_file = True\n",
        "                        else: # Exact match search logic (no wildcard)\n",
        "                            if case_insensitive_search:\n",
        "                                if tag.lower() == search_tag.lower():\n",
        "                                    tag_occurrences[search_tag] += 1\n",
        "                                    found_in_file = True\n",
        "                                    break # No need to count multiple times in same file (case-insensitive)\n",
        "                            else:\n",
        "                                if tag == search_tag:\n",
        "                                    tag_occurrences[search_tag] += 1\n",
        "                                    found_in_file = True\n",
        "                                    break # No need to count multiple times in same file (case-sensitive)\n",
        "                    if list_files_with_tags and found_in_file:\n",
        "                        files_with_tags[search_tag][filename] = file_matched_tags # MODIFIED: Store filename and matched_tags\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing tag file {filename}: {e}\")\n",
        "\n",
        "    print(f\"\\n--- {EMOJI_SEARCH} Tag Search Summary {EMOJI_SEARCH} ---\")\n",
        "    print(f\"Files analyzed: {files_analyzed_count}\")\n",
        "    for search_tag, count in tag_occurrences.items():\n",
        "        print(f\"\\n{EMOJI_TAG} Tag: {COLOR_HIGHLIGHT}{search_tag}{COLOR_RESET}\") # Emoji and color added\n",
        "        print(f\"  Occurrences: {count}\")\n",
        "        if \"*\" in search_tag: # NEW: Display matched tags for wildcard searches\n",
        "            matched_tags = matched_tags_list[search_tag]\n",
        "            if matched_tags:\n",
        "                print(f\"  Matched tags: {', '.join(matched_tags)}\") # NEW: List matched tags\n",
        "        if list_files_with_tags:\n",
        "            files_and_tags = files_with_tags[search_tag] # MODIFIED: Get files_with_tags dict\n",
        "            if files_and_tags:\n",
        "                print(f\"  Found in files ({len(files_and_tags)}):\")\n",
        "                for file, matched_tags in files_and_tags.items(): # MODIFIED: Iterate through files_and_tags.items()\n",
        "                    print(f\"  - {EMOJI_FILE} {file}: Tags: {', '.join(matched_tags)}\") # MODIFIED: Display filename and matched tags\n",
        "            else:\n",
        "                print(\"  Found in no files.\") # Should not happen if count > 0, but for completeness\n",
        "\n",
        "    print(f\"\\n{EMOJI_CHECKMARK} Tag search process completed.\")\n",
        "\n",
        "\n",
        "# Run tag search with Colab parameters\n",
        "search_tags_in_directory(directory=full_directory, search_tags=search_tags, case_insensitive_search=case_insensitive_search, list_files_with_tags=list_files_with_tags)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8UclSa9hVHeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🔢 Count Dataset\n",
        "\n",
        "import os\n",
        "\n",
        "# No need to import drive or define directories again\n",
        "# Define directory using the initialized full_directory\n",
        "# full_directory is assumed to be defined in the \"Initialization Cell\"\n",
        "\n",
        "# --- Emojis ---\n",
        "EMOJI_SUMMARY = \"📊\"\n",
        "EMOJI_FILES = \"🗂️\"\n",
        "EMOJI_IMAGES = \"🖼️\"\n",
        "EMOJI_TAGS = \"🏷️\"\n",
        "EMOJI_SIZE = \"💾\"\n",
        "\n",
        "def count_dataset(directory=full_directory): # Use full_directory directly\n",
        "    \"\"\"\n",
        "    Counts and summarizes the files in the specified directory,\n",
        "    including image files, text files, and the total size of image files, with enhanced output.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        print(f\"Directory not found: {directory}\")\n",
        "        return\n",
        "\n",
        "    image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp']\n",
        "    image_count = 0\n",
        "    text_count = 0\n",
        "    total_image_size = 0\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "        if os.path.isfile(file_path):\n",
        "            is_image = False\n",
        "            for ext in image_extensions:\n",
        "                if filename.lower().endswith(ext):\n",
        "                    is_image = True\n",
        "                    break\n",
        "            if is_image:\n",
        "                image_count += 1\n",
        "                total_image_size += os.path.getsize(file_path)\n",
        "            elif filename.endswith(\".txt\"):\n",
        "                text_count += 1\n",
        "\n",
        "    print(f\"\\n--- {EMOJI_SUMMARY} Dataset Summary {EMOJI_SUMMARY} ---\")\n",
        "    print(f\"{EMOJI_FILES} Directory: {directory}\")\n",
        "    print(f\"{EMOJI_FILES} Total Files: {image_count + text_count}\")\n",
        "    print(f\"{EMOJI_IMAGES} Image Files: {image_count}\")\n",
        "    print(f\"{EMOJI_TAGS} Tag Files (text): {text_count}\")\n",
        "\n",
        "    # Convert bytes to human-readable format (e.g., KB, MB, GB)\n",
        "    size_in_gb = total_image_size / (1024**3)\n",
        "    size_in_mb = total_image_size / (1024**2)\n",
        "    size_in_kb = total_image_size / 1024\n",
        "\n",
        "    if size_in_gb >= 1:\n",
        "        print(f\"{EMOJI_SIZE} Total Image Size: {size_in_gb:.2f} GB\")\n",
        "    elif size_in_mb >= 1:\n",
        "        print(f\"{EMOJI_SIZE} Total Image Size: {size_in_mb:.2f} MB\")\n",
        "    elif size_in_kb >= 1:\n",
        "        print(f\"{EMOJI_SIZE} Total Image Size: {size_in_kb:.2f} KB\")\n",
        "    else:\n",
        "        print(f\"{EMOJI_SIZE} Total Image Size: {total_image_size} bytes\")\n",
        "\n",
        "# Run the dataset counting function\n",
        "count_dataset(directory=full_directory) # Use full_directory"
      ],
      "metadata": {
        "cellView": "form",
        "id": "f-C7mHj-qB_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🚚 Move or Copy Dataset Files\n",
        "\n",
        "import os\n",
        "import shutil # Import shutil for copy operation\n",
        "\n",
        "# No need to import drive or define directories again\n",
        "# Define directory using the initialized full_directory\n",
        "# full_directory is assumed to be defined in the \"Initialization Cell\"\n",
        "\n",
        "# Colab Parameters\n",
        "destination_path = \"/content/drive/MyDrive/Path/To/Somewhere\" #@param {type:\"string\"}\n",
        "copy_files = False #@param {type:\"boolean\"}\n",
        "\n",
        "def move_copy_dataset(source_dir=full_directory, dest_dir=\"\", copy_files=False):\n",
        "    \"\"\"\n",
        "    Moves or copies image and tag files from the source directory to the destination directory.\n",
        "\n",
        "    Args:\n",
        "        source_dir (str): Path to the source directory (default: full_directory).\n",
        "        dest_dir (str): Path to the destination directory in Google Drive.\n",
        "        copy_files (bool): If True, files are copied; if False, files are moved.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(source_dir):\n",
        "        print(f\"Error: Source directory not found: {source_dir}\")\n",
        "        return\n",
        "    if not dest_dir:\n",
        "        print(\"Error: Destination path is empty. Please provide a valid Google Drive path.\")\n",
        "        return\n",
        "\n",
        "    os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "    operation_type = \"Copying\" if copy_files else \"Moving\"\n",
        "    print(f\"\\n--- {operation_type} Dataset Files ---\")\n",
        "    print(f\"Source directory: {source_dir}\")\n",
        "    print(f\"Destination directory: {dest_dir}\")\n",
        "\n",
        "    files_processed_count = 0\n",
        "    errors_count = 0\n",
        "\n",
        "    for filename in os.listdir(source_dir):\n",
        "        source_file_path = os.path.join(source_dir, filename)\n",
        "        dest_file_path = os.path.join(dest_dir, filename)\n",
        "\n",
        "        if os.path.isfile(source_file_path):\n",
        "            try:\n",
        "                if copy_files:\n",
        "                    shutil.copy2(source_file_path, dest_file_path)\n",
        "                else:\n",
        "                    shutil.move(source_file_path, dest_file_path)\n",
        "                files_processed_count += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing file {filename}: {e}\")\n",
        "                errors_count += 1\n",
        "\n",
        "    print(\"\\n--- Move/Copy Summary ---\")\n",
        "    print(f\"Operation type: {operation_type}\")\n",
        "    print(f\"Files processed: {files_processed_count}\")\n",
        "    if errors_count > 0:\n",
        "        print(f\"Errors encountered: {errors_count}\")\n",
        "    print(f\"✅ Dataset files {operation_type.lower()} completed.\")\n",
        "\n",
        "\n",
        "# Run move/copy operation based on Colab parameters\n",
        "move_copy_dataset(source_dir=full_directory, dest_dir=destination_path, copy_files=copy_files)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1NKQqYv77crJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🚮 Clean Folder\n",
        "\n",
        "import os\n",
        "# No need to import drive or define directories again\n",
        "\n",
        "# Feature Checkboxes for folder cleaning\n",
        "remove_non_image_files = True #@param {type:\"boolean\"}\n",
        "delete_all_files = True #@param {type:\"boolean\"}\n",
        "\n",
        "# --- Emojis ---\n",
        "EMOJI_FOLDER = \"🗂️\"\n",
        "EMOJI_DELETE = \"🗑️\"\n",
        "EMOJI_NON_IMAGE = \"📄\"\n",
        "EMOJI_ALL_FILES = \"💥\"\n",
        "EMOJI_WARNING = \"⚠️\"\n",
        "EMOJI_CHECKMARK = \"✅\"\n",
        "\n",
        "def clean_folder(directory=full_directory, remove_non_images=False, delete_all=False):\n",
        "    \"\"\"\n",
        "    Cleans the specified directory by removing non-image files or all files, with enhanced output.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        print(f\"{EMOJI_WARNING} Directory not found: {directory}\")\n",
        "        return\n",
        "\n",
        "    if delete_all:\n",
        "        print(f\"{EMOJI_WARNING} Deleting ALL files in: {directory} - USE WITH CAUTION! {EMOJI_ALL_FILES}\")\n",
        "        file_count = 0\n",
        "        for filename in os.listdir(directory):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            try:\n",
        "                if os.path.isfile(file_path):\n",
        "                    os.remove(file_path)\n",
        "                    file_count += 1\n",
        "            except OSError as e:\n",
        "                print(f\"Error deleting {filename}: {e}\")\n",
        "        print(f\"{EMOJI_CHECKMARK} Deleted {file_count} files in total (all files). {EMOJI_DELETE}\")\n",
        "\n",
        "    elif remove_non_images:\n",
        "        print(f\"{EMOJI_DELETE} Removing non-image files from: {directory} {EMOJI_NON_IMAGE}\")\n",
        "        image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp']\n",
        "        non_image_files_deleted = 0\n",
        "\n",
        "        for filename in os.listdir(directory):\n",
        "            is_image = False\n",
        "            for ext in image_extensions:\n",
        "                if filename.lower().endswith(ext):\n",
        "                    is_image = True\n",
        "                    break\n",
        "            if not is_image and os.path.isfile(os.path.join(directory, filename)): # Check if it's a file and not an image\n",
        "                try:\n",
        "                    os.remove(os.path.join(directory, filename))\n",
        "                    non_image_files_deleted += 1\n",
        "                    print(f\"Deleted non-image file: {filename}\")\n",
        "                except OSError as e:\n",
        "                    print(f\"Error deleting {filename}: {e}\")\n",
        "        print(f\"{EMOJI_CHECKMARK} Deleted {non_image_files_deleted} non-image files. {EMOJI_NON_IMAGE}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"{EMOJI_FOLDER} No cleaning options selected. Check the checkboxes to perform cleaning actions.\")\n",
        "\n",
        "# Run the folder cleaning function based on checkbox selections\n",
        "clean_folder(directory=full_directory, remove_non_images=remove_non_image_files, delete_all=delete_all_files) # Use full_directory"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jNv4L7V2pNMC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}